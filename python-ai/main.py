import asyncio
import logging
from concurrent import futures
import os
import time
import hashlib
from typing import Dict, Optional, Tuple

import grpc
import openai  # å¼•å…¥openaiåº“
from dotenv import load_dotenv

# å¯¼å…¥æˆ‘ä»¬ç”Ÿæˆçš„gRPCä»£ç 
import intelligence_pb2
import intelligence_pb2_grpc

# å¯¼å…¥å¯¹è¯çŠ¶æ€ç®¡ç†å™¨
from dialogue_state import DialogueStateManager, ChatMessage

# --- ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ ---
load_dotenv()

# --- OpenAIå®¢æˆ·ç«¯ä¼˜åŒ–é…ç½® ---
# åˆ›å»ºä¸€ä¸ªOpenAIå®¢æˆ·ç«¯ï¼Œä¼˜åŒ–è¿æ¥æ± å’Œè¶…æ—¶è®¾ç½®
client = openai.OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE_URL"),
    max_retries=2,  # å‡å°‘é‡è¯•æ¬¡æ•°ä»¥æé«˜å“åº”é€Ÿåº¦
    timeout=15.0,   # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
)
# -------------------------

# AIå“åº”ç¼“å­˜ç±»
class AIResponseCache:
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        """
        åˆå§‹åŒ–AIå“åº”ç¼“å­˜
        :param max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        :param ttl_seconds: ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.cache: Dict[str, Tuple[intelligence_pb2.InterpretResponse, float]] = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
    
    def _generate_cache_key(self, user_text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(user_text.encode('utf-8')).hexdigest()
    
    def _is_expired(self, timestamp: float) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        return time.time() - timestamp > self.ttl_seconds
    
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
    
    def get(self, user_text: str) -> Optional[intelligence_pb2.InterpretResponse]:
        """ä»ç¼“å­˜ä¸­è·å–å“åº”"""
        cache_key = self._generate_cache_key(user_text)
        
        logging.info(f"æ£€æŸ¥ç¼“å­˜: {user_text[:30]}... (ç¼“å­˜é”®: {cache_key[:8]}...)")
        
        if cache_key in self.cache:
            response, timestamp = self.cache[cache_key]
            if not self._is_expired(timestamp):
                logging.info(f"ç¼“å­˜å‘½ä¸­: {user_text[:30]}...")
                return response
            else:
                # ç§»é™¤è¿‡æœŸç¼“å­˜
                logging.info(f"ç¼“å­˜è¿‡æœŸï¼Œç§»é™¤: {user_text[:30]}...")
                del self.cache[cache_key]
        else:
            logging.info(f"ç¼“å­˜æœªå‘½ä¸­: {user_text[:30]}...")
        
        return None
    
    def put(self, user_text: str, response: intelligence_pb2.InterpretResponse):
        """å°†å“åº”å­˜å…¥ç¼“å­˜"""
        cache_key = self._generate_cache_key(user_text)
        
        logging.info(f"å‡†å¤‡å­˜å‚¨ç¼“å­˜: {user_text[:30]}... (ç¼“å­˜é”®: {cache_key[:8]}...)")
        
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œå…ˆæ¸…ç†è¿‡æœŸé¡¹
        if len(self.cache) >= self.max_size:
            self._cleanup_expired()
            
            # å¦‚æœæ¸…ç†åä»ç„¶æ»¡äº†ï¼Œç§»é™¤æœ€æ—§çš„æ¡ç›®
            if len(self.cache) >= self.max_size:
                oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
                del self.cache[oldest_key]
        
        # åˆ›å»ºå“åº”çš„æ·±æ‹·è´ä»¥é¿å…å¼•ç”¨é—®é¢˜
        cached_response = intelligence_pb2.InterpretResponse()
        cached_response.intent = response.intent
        cached_response.structured_data_json = response.structured_data_json
        
        self.cache[cache_key] = (cached_response, time.time())
        logging.info(f"ç¼“å­˜å­˜å‚¨æˆåŠŸ: {user_text[:30]}... (ç¼“å­˜å¤§å°: {len(self.cache)})")

# å…¨å±€ç¼“å­˜å®ä¾‹
ai_cache = AIResponseCache(max_size=500, ttl_seconds=1800)  # 30åˆ†é’ŸTTL

# --- gRPC Service Implementation ---
# ... main.py æ–‡ä»¶å‰é¢çš„éƒ¨åˆ†ä¿æŒä¸å˜ ...

# --- gRPC Service Implementation ---
class IntelligenceServiceImpl(intelligence_pb2_grpc.IntelligenceServiceServicer):
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡ï¼Œè®¾ç½®å·¥ä½œçº¿ç¨‹æ± å’Œå¯¹è¯çŠ¶æ€ç®¡ç†å™¨"""
        self.executor = futures.ThreadPoolExecutor(max_workers=20)
        
        # åˆå§‹åŒ–å¯¹è¯çŠ¶æ€ç®¡ç†å™¨
        try:
            redis_host = os.getenv("REDIS_HOST", "localhost")
            redis_port = int(os.getenv("REDIS_PORT", "6379"))
            self.dialogue_manager = DialogueStateManager(
                redis_host=redis_host,
                redis_port=redis_port,
                session_ttl=1800,  # 30åˆ†é’Ÿ
                max_history_length=20
            )
            logging.info("âœ… DialogueStateManager initialized successfully")
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to initialize DialogueStateManager: {e}")
            logging.warning("âš ï¸ Running without persistent dialogue state")
            self.dialogue_manager = None
    
    def InterpretText(self, request: intelligence_pb2.InterpretRequest, context):
        start_time = time.time()
        logging.info(f"Received text: '{request.user_text}' from session '{request.session_id}'")
        
        # éªŒè¯è¾“å…¥ - æ‹’ç»ç©ºè¾“å…¥
        if not request.user_text or request.user_text.strip() == "":
            logging.warning("Empty input rejected")
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("User text cannot be empty")
            return intelligence_pb2.InterpretResponse()
        
        # éªŒè¯è¾“å…¥é•¿åº¦ - æ‹’ç»è¿‡é•¿è¾“å…¥  
        if len(request.user_text) > 5000:
            logging.warning(f"Input too long: {len(request.user_text)} characters")
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("User text is too long (max 5000 characters)")
            return intelligence_pb2.InterpretResponse()

        # è·å–å¯¹è¯å†å²ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        conversation_history = []
        if self.dialogue_manager:
            try:
                # ç¡®ä¿ä¼šè¯å­˜åœ¨
                self.dialogue_manager.create_session(request.session_id)
                
                # è·å–å¯¹è¯å†å²
                history_messages = self.dialogue_manager.get_conversation_history(
                    request.session_id, limit=10
                )
                
                # è½¬æ¢ä¸ºOpenAIæ¶ˆæ¯æ ¼å¼
                for msg in history_messages:
                    conversation_history.append({
                        "role": msg.role,
                        "content": msg.content
                    })
                
                logging.info(f"Retrieved {len(conversation_history)} messages from history")
                
            except Exception as e:
                logging.warning(f"Failed to get conversation history: {e}")

        # æ£€æŸ¥ç®€å•ç¼“å­˜
        cached_response = ai_cache.get(request.user_text)
        if cached_response is not None and len(conversation_history) == 0:
            logging.info(f"è¿”å›ç¼“å­˜ç»“æœ: {request.user_text[:30]}...")
            return cached_response

        # AIå·¥å…·å®šä¹‰
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_employee_manager",
                    "description": "Get the manager of a specified employee",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "employee_id": { "type": "string", "description": "The UUID of the employee" }
                        },
                        "required": ["employee_id"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "update_phone_number",
                    "description": "Update an employee's phone number",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "employee_id": {
                                "type": "string",
                                "description": "The UUID of the employee whose phone number is to be updated",
                            },
                            "new_phone_number": {
                                "type": "string",
                                "description": "The new phone number",
                            }
                        },
                        "required": ["employee_id", "new_phone_number"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "list_employees",
                    "description": "List employees with optional search criteria",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "search": {
                                "type": "string",
                                "description": "Search term for employee name or number"
                            },
                            "page": {
                                "type": "integer",
                                "description": "Page number (default: 1)"
                            }
                        }
                    },
                },
            }
        ]

        try:
            # æ„å»ºåŒ…å«å†å²çš„æ¶ˆæ¯åˆ—è¡¨
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„HRç³»ç»Ÿæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨è¯†åˆ«ç”¨æˆ·çš„HRç›¸å…³æ„å›¾ã€‚

ä½ æœ‰è®°å¿†èƒ½åŠ›ï¼Œå¯ä»¥è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡è¿›è¡Œå›å¤ã€‚

æ ¸å¿ƒæ„å›¾ç±»å‹å’Œå¯¹åº”çš„å‡½æ•°ï¼š
1. list_employees - æŸ¥è¯¢å‘˜å·¥åˆ—è¡¨ (å…³é”®è¯: æŸ¥è¯¢ã€æŸ¥çœ‹ã€å‘˜å·¥ã€åˆ—è¡¨ã€æœç´¢)
2. update_phone_number - æ›´æ–°ç”µè¯å·ç  (å…³é”®è¯: æ›´æ–°ã€ä¿®æ”¹ã€ç”µè¯ã€æ‰‹æœºã€å·ç )
3. get_employee_manager - æŸ¥çœ‹ç»ç†ä¿¡æ¯ (å…³é”®è¯: ç»ç†ã€ä¸Šçº§ã€ä¸»ç®¡ã€é¢†å¯¼)

è¯†åˆ«è§„åˆ™ï¼š
- ä»”ç»†åˆ†æç”¨æˆ·è¾“å…¥çš„å…³é”®è¯å’Œä¸Šä¸‹æ–‡
- ç»“åˆä¹‹å‰çš„å¯¹è¯å†å²ç†è§£ç”¨æˆ·æ„å›¾
- å¦‚æœç”¨æˆ·è¯¢é—®æˆ–æœç´¢å‘˜å·¥ï¼Œé€‰æ‹©list_employees
- å¦‚æœç”¨æˆ·æåˆ°æ›´æ–°ã€ä¿®æ”¹ç”µè¯å·ç ï¼Œé€‰æ‹©update_phone_number
- å¦‚æœç”¨æˆ·è¯¢é—®ç»ç†ã€ä¸Šçº§ä¿¡æ¯ï¼Œé€‰æ‹©get_employee_manager
- æå–ç›¸å…³çš„ç»“æ„åŒ–æ•°æ®å‚æ•°

è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²è¯†åˆ«æ„å›¾å¹¶è°ƒç”¨å¯¹åº”å‡½æ•°ã€‚"""

            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
            messages.extend(conversation_history[-10:])
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": request.user_text})

            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=messages,
                tools=tools,
                tool_choice="auto",
                temperature=0.1,
                max_tokens=512,
                stream=False,
            )

            response_message = response.choices[0].message
            tool_calls = response_message.tool_calls
            
            # å¤„ç†AIå“åº”
            if tool_calls:
                function_name = tool_calls[0].function.name
                function_args_json = tool_calls[0].function.arguments
                assistant_content = f"æˆ‘è¯†åˆ«åˆ°æ‚¨çš„æ„å›¾æ˜¯ï¼š{function_name}ï¼Œæ­£åœ¨ä¸ºæ‚¨å¤„ç†..."

                logging.info(f"LLM wants to call function: {function_name} with args: {function_args_json}")
            else:
                function_name = "no_intent_detected"
                function_args_json = "{}"
                assistant_content = response_message.content or "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„å›¾ï¼Œè¯·å°è¯•é‡æ–°è¡¨è¾¾ã€‚"

            # ä¿å­˜å¯¹è¯åˆ°Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.dialogue_manager:
                try:
                    user_message = ChatMessage(
                        role="user",
                        content=request.user_text,
                        timestamp=start_time,
                        intent=function_name
                    )
                    
                    assistant_message = ChatMessage(
                        role="assistant",
                        content=assistant_content,
                        timestamp=time.time(),
                        intent=function_name
                    )
                    
                    context_updates = {
                        "last_intent": function_name,
                        "last_activity": time.time(),
                        "processing_time": time.time() - start_time
                    }
                    
                    self.dialogue_manager.save_conversation_turn(
                        request.session_id,
                        user_message,
                        assistant_message,
                        context_updates
                    )
                    
                except Exception as e:
                    logging.warning(f"Failed to save conversation to Redis: {e}")

            # åˆ›å»ºå“åº”å¯¹è±¡
            result_response = intelligence_pb2.InterpretResponse(
                intent=function_name,
                structured_data_json=function_args_json
            )
            
            # ç®€å•ç¼“å­˜ä»…ç”¨äºæ— å†å²çš„å•æ¬¡æŸ¥è¯¢
            if len(conversation_history) == 0:
                ai_cache.put(request.user_text, result_response)
            
            processing_time = time.time() - start_time
            logging.info(f"Request processed in {processing_time:.3f}s: {function_name}")
            
            return result_response

        except Exception as e:
            logging.error(f"Error calling OpenAI: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"Error communicating with LLM: {e}")
            return intelligence_pb2.InterpretResponse()

# ... æ–‡ä»¶åé¢çš„ serve() å’Œ main() å‡½æ•°ä¿æŒä¸å˜ ...
async def serve() -> None:
    port = "50051"
    # å¢åŠ gRPCæœåŠ¡å™¨çš„å¹¶å‘å¤„ç†èƒ½åŠ›
    options = [
        ('grpc.keepalive_time_ms', 30000),
        ('grpc.keepalive_timeout_ms', 5000),
        ('grpc.keepalive_permit_without_calls', True),
        ('grpc.http2.max_pings_without_data', 0),  
        ('grpc.http2.min_time_between_pings_ms', 10000),
        ('grpc.http2.min_ping_interval_without_data_ms', 300000),
        ('grpc.max_connection_idle_ms', 60000),
    ]
    
    server = grpc.aio.server(
        futures.ThreadPoolExecutor(max_workers=50),
        options=options
    )
    intelligence_pb2_grpc.add_IntelligenceServiceServicer_to_server(
        IntelligenceServiceImpl(), server
    )
    
    server.add_insecure_port(f"[::]:{port}")
    logging.info(f"ğŸ§™ Python AI Service 'The Wizard Tower' is listening on gRPC port {port}")
    await server.start()
    await server.wait_for_termination()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(serve())