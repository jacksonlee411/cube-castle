#!/usr/bin/env python3
"""
AIæœåŠ¡å…¨é¢æ€§èƒ½æµ‹è¯•å·¥å…·
"""

import time
import statistics
import grpc
import sys
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import intelligence_pb2
import intelligence_pb2_grpc

def measure_single_request_performance():
    """æµ‹é‡å•ä¸ªè¯·æ±‚çš„æ€§èƒ½"""
    print("ğŸ” å•è¯·æ±‚æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    channel = grpc.insecure_channel('localhost:50051')
    stub = intelligence_pb2_grpc.IntelligenceServiceStub(channel)
    
    test_cases = [
        "æ›´æ–°æˆ‘çš„ç”µè¯å·ç ä¸º13800138000",
        "è°æ˜¯æˆ‘çš„ç»ç†ï¼Ÿ",
        "æˆ‘æƒ³æŸ¥çœ‹æˆ‘çš„ä¸ªäººä¿¡æ¯",
        "å¸®æˆ‘ç”³è¯·å¹´å‡",
        "æŸ¥è¯¢å…¬å¸ç»„ç»‡æ¶æ„",
        "æŸ¥çœ‹æˆ‘çš„å·¥èµ„å•",
        "ç”³è¯·è°ƒå²—åˆ°æŠ€æœ¯éƒ¨",
        "æŸ¥è¯¢åŸ¹è®­è¯¾ç¨‹"
    ]
    
    response_times = []
    
    for i, text in enumerate(test_cases, 1):
        print(f"æµ‹è¯• {i}/{len(test_cases)}: {text[:20]}...")
        
        start_time = time.time()
        try:
            request = intelligence_pb2.InterpretRequest()
            request.user_text = text
            request.session_id = f"single-test-{i}"
            
            response = stub.InterpretText(request, timeout=30)
            end_time = time.time()
            
            response_time = (end_time - start_time) * 1000
            response_times.append(response_time)
            
            print(f"  âœ… å“åº”æ—¶é—´: {response_time:.0f}ms")
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    channel.close()
    
    if response_times:
        print(f"\nğŸ“Š å•è¯·æ±‚æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.0f}ms")
        print(f"  æœ€çŸ­å“åº”æ—¶é—´: {min(response_times):.0f}ms")
        print(f"  æœ€é•¿å“åº”æ—¶é—´: {max(response_times):.0f}ms")
        print(f"  å“åº”æ—¶é—´ä¸­ä½æ•°: {statistics.median(response_times):.0f}ms")
        if len(response_times) > 1:
            print(f"  æ ‡å‡†å·®: {statistics.stdev(response_times):.0f}ms")
    
    return response_times

def make_concurrent_request(session_id, text):
    """å‘èµ·å•ä¸ªå¹¶å‘è¯·æ±‚"""
    try:
        channel = grpc.insecure_channel('localhost:50051')
        stub = intelligence_pb2_grpc.IntelligenceServiceStub(channel)
        
        start_time = time.time()
        
        request = intelligence_pb2.InterpretRequest()
        request.user_text = text
        request.session_id = f"concurrent-{session_id}"
        
        response = stub.InterpretText(request, timeout=30)
        end_time = time.time()
        
        response_time = (end_time - start_time) * 1000
        channel.close()
        
        return {
            'session_id': session_id,
            'response_time': response_time,
            'success': True,
            'intent': response.intent
        }
    except Exception as e:
        return {
            'session_id': session_id, 
            'response_time': None,
            'success': False,
            'error': str(e)
        }

def measure_concurrent_performance():
    """æµ‹é‡å¹¶å‘æ€§èƒ½"""
    print("\nğŸš€ å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
    concurrency_levels = [5, 10, 20]
    
    test_texts = [
        "æ›´æ–°æˆ‘çš„ç”µè¯å·ç ä¸º13800138000",
        "è°æ˜¯æˆ‘çš„ç»ç†ï¼Ÿ",
        "æˆ‘æƒ³æŸ¥çœ‹æˆ‘çš„ä¸ªäººä¿¡æ¯",
        "å¸®æˆ‘ç”³è¯·å¹´å‡",
        "æŸ¥è¯¢å…¬å¸ç»„ç»‡æ¶æ„"
    ]
    
    for concurrency in concurrency_levels:
        print(f"\nğŸ“ˆ å¹¶å‘çº§åˆ«: {concurrency}")
        print("-" * 30)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = []
        for i in range(concurrency):
            text = test_texts[i % len(test_texts)]
            test_data.append((i, text))
        
        start_total = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            future_to_session = {
                executor.submit(make_concurrent_request, session_id, text): session_id 
                for session_id, text in test_data
            }
            
            results = []
            for future in as_completed(future_to_session):
                result = future.result()
                results.append(result)
        
        end_total = time.time()
        total_time = (end_total - start_total) * 1000
        
        # ç»Ÿè®¡ç»“æœ
        successful_results = [r for r in results if r['success']]
        failed_count = len(results) - len(successful_results)
        
        if successful_results:
            response_times = [r['response_time'] for r in successful_results]
            
            print(f"  æ€»ç”¨æ—¶: {total_time:.0f}ms")
            print(f"  æˆåŠŸè¯·æ±‚: {len(successful_results)}/{concurrency}")
            print(f"  å¤±è´¥è¯·æ±‚: {failed_count}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.0f}ms")
            print(f"  æœ€å¤§å“åº”æ—¶é—´: {max(response_times):.0f}ms")
            print(f"  ååé‡: {len(successful_results) / (total_time / 1000):.1f} req/s")
        else:
            print(f"  âŒ æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†")

def measure_cache_performance():
    """æµ‹é‡ç¼“å­˜æ€§èƒ½"""
    print("\nğŸ’¾ ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    channel = grpc.insecure_channel('localhost:50051')
    stub = intelligence_pb2_grpc.IntelligenceServiceStub(channel)
    
    test_text = "ç¼“å­˜æ€§èƒ½æµ‹è¯•è¯·æ±‚"
    
    # ç¬¬ä¸€æ¬¡è¯·æ±‚ - åº”è¯¥è°ƒç”¨AIæ¨¡å‹
    print("ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆæ— ç¼“å­˜ï¼‰:")
    start_time = time.time()
    
    request = intelligence_pb2.InterpretRequest()
    request.user_text = test_text
    request.session_id = "cache-test-1"
    
    response = stub.InterpretText(request, timeout=30)
    end_time = time.time()
    
    first_time = (end_time - start_time) * 1000
    print(f"  å“åº”æ—¶é—´: {first_time:.0f}ms")
    
    # ç¬¬äºŒæ¬¡è¯·æ±‚ - åº”è¯¥å‘½ä¸­ç¼“å­˜
    print("ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰:")
    start_time = time.time()
    
    request = intelligence_pb2.InterpretRequest()
    request.user_text = test_text
    request.session_id = "cache-test-2"
    
    response = stub.InterpretText(request, timeout=30)
    end_time = time.time()
    
    second_time = (end_time - start_time) * 1000
    print(f"  å“åº”æ—¶é—´: {second_time:.0f}ms")
    
    # è®¡ç®—ç¼“å­˜æ•ˆæœ
    if first_time > 0:
        improvement = ((first_time - second_time) / first_time) * 100
        print(f"\nğŸ“Š ç¼“å­˜æ•ˆæœåˆ†æ:")
        print(f"  æ€§èƒ½æå‡: {improvement:.1f}%")
        print(f"  åŠ é€Ÿæ¯”: {first_time / second_time:.1f}x")
    
    channel.close()

def run_comprehensive_test():
    """è¿è¡Œå…¨é¢æ€§èƒ½æµ‹è¯•"""
    print("ğŸ° Cube Castle - AIæœåŠ¡å…¨é¢æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # 1. å•è¯·æ±‚æ€§èƒ½æµ‹è¯•
    single_response_times = measure_single_request_performance()
    
    # 2. å¹¶å‘æ€§èƒ½æµ‹è¯•
    measure_concurrent_performance()
    
    # 3. ç¼“å­˜æ€§èƒ½æµ‹è¯•
    measure_cache_performance()
    
    # 4. æ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    if single_response_times:
        avg_time = statistics.mean(single_response_times)
        print(f"å¹³å‡å•è¯·æ±‚å“åº”æ—¶é—´: {avg_time:.0f}ms")
        
        if avg_time < 2000:
            print("âœ… æ€§èƒ½ç›®æ ‡è¾¾æˆï¼šå¹³å‡å“åº”æ—¶é—´ < 2000ms")
        else:
            needed_improvement = ((avg_time - 2000) / avg_time) * 100
            print(f"âŒ éœ€è¦æ”¹è¿› {needed_improvement:.1f}% ä»¥è¾¾åˆ°ç›®æ ‡")
    
    print("âœ… å…¨é¢æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    try:
        run_comprehensive_test()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)