package main

import (
	"context"
	"crypto/md5"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
	"github.com/redis/go-redis/v9"
)

// CDCäº‹ä»¶æ¨¡å‹
type CDCOrganizationEvent struct {
	Before *CDCOrganizationData `json:"before"`
	After  *CDCOrganizationData `json:"after"`
	Source CDCSource            `json:"source"`
	Op     string               `json:"op"` // c, u, d, r
	TsMs   int64                `json:"ts_ms"`
}

type CDCOrganizationData struct {
	ID         *string `json:"id"`
	TenantID   *string `json:"tenant_id"`
	Code       *string `json:"code"`
	ParentCode *string `json:"parent_code"`
	Name       *string `json:"name"`
	UnitType   *string `json:"unit_type"`
	Status     *string `json:"status"`
	Level      *int    `json:"level"`
	Path       *string `json:"path"`
	SortOrder  *int    `json:"sort_order"`
	Description *string `json:"description"`
	CreatedAt  *time.Time `json:"created_at"`
	UpdatedAt  *time.Time `json:"updated_at"`
}

type CDCSource struct {
	Version   string `json:"version"`
	Connector string `json:"connector"`
	Name      string `json:"name"`
	TsMs      int64  `json:"ts_ms"`
	Snapshot  string `json:"snapshot"`
	DB        string `json:"db"`
	Schema    string `json:"schema"`
	Table     string `json:"table"`
	TxID      int64  `json:"txId"`
	LSN       int64  `json:"lsn"`
}

// ç¼“å­˜å¤±æ•ˆæœåŠ¡
type CacheInvalidator struct {
	redisClient *redis.Client
	consumer    *kafka.Consumer
	logger      *log.Logger
}

func NewCacheInvalidator(redisAddr, redisPassword string, kafkaBrokers []string, groupID string, logger *log.Logger) (*CacheInvalidator, error) {
	// Redisè¿æ¥
	redisClient := redis.NewClient(&redis.Options{
		Addr:     redisAddr,
		Password: redisPassword,
		DB:       0,
	})

	// æµ‹è¯•Redisè¿æ¥
	_, err := redisClient.Ping(context.Background()).Result()
	if err != nil {
		return nil, fmt.Errorf("Redisè¿æ¥å¤±è´¥: %w", err)
	}

	// Kafkaæ¶ˆè´¹è€…é…ç½®
	config := &kafka.ConfigMap{
		"bootstrap.servers": strings.Join(kafkaBrokers, ","),
		"group.id":          groupID,
		"auto.offset.reset": "latest",
		"enable.auto.commit": true,
		"auto.commit.interval.ms": 1000,
	}

	consumer, err := kafka.NewConsumer(config)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºKafkaæ¶ˆè´¹è€…å¤±è´¥: %w", err)
	}

	return &CacheInvalidator{
		redisClient: redisClient,
		consumer:    consumer,
		logger:      logger,
	}, nil
}

// ç”Ÿæˆç¼“å­˜é”® - ä¸GraphQLæœåŠ¡ä¿æŒä¸€è‡´
func (c *CacheInvalidator) getCacheKey(operation string, params ...interface{}) string {
	h := md5.New()
	h.Write([]byte(fmt.Sprintf("org:%s:%v", operation, params)))
	return fmt.Sprintf("cache:%x", h.Sum(nil))
}

// å¤±æ•ˆç›¸å…³ç¼“å­˜
func (c *CacheInvalidator) invalidateOrganizationCaches(ctx context.Context, tenantID string, affectedCode string) error {
	// éœ€è¦å¤±æ•ˆçš„ç¼“å­˜æ¨¡å¼
	patterns := []string{
		"cache:*", // å¤±æ•ˆæ‰€æœ‰ç»„ç»‡ç›¸å…³ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
	}

	totalInvalidated := 0
	for _, pattern := range patterns {
		keys, err := c.redisClient.Keys(ctx, pattern).Result()
		if err != nil {
			c.logger.Printf("è·å–ç¼“å­˜é”®å¤±è´¥ï¼Œæ¨¡å¼: %s, é”™è¯¯: %v", pattern, err)
			continue
		}

		if len(keys) > 0 {
			deleted, err := c.redisClient.Del(ctx, keys...).Result()
			if err != nil {
				c.logger.Printf("åˆ é™¤ç¼“å­˜å¤±è´¥ï¼Œé”®æ•°é‡: %d, é”™è¯¯: %v", len(keys), err)
				continue
			}
			totalInvalidated += int(deleted)
			c.logger.Printf("ç¼“å­˜å¤±æ•ˆæˆåŠŸ - æ¨¡å¼: %s, åˆ é™¤: %d ä¸ªç¼“å­˜é¡¹", pattern, deleted)
		}
	}

	if totalInvalidated > 0 {
		c.logger.Printf("âœ… ç¼“å­˜å¤±æ•ˆå®Œæˆ - ç§Ÿæˆ·: %s, å½±å“ç»„ç»‡: %s, æ€»è®¡å¤±æ•ˆ: %d ä¸ªç¼“å­˜é¡¹", 
			tenantID, affectedCode, totalInvalidated)
	} else {
		c.logger.Printf("â„¹ï¸ æœªæ‰¾åˆ°éœ€è¦å¤±æ•ˆçš„ç¼“å­˜ - ç§Ÿæˆ·: %s, å½±å“ç»„ç»‡: %s", tenantID, affectedCode)
	}

	return nil
}

// å¤„ç†CDCäº‹ä»¶
func (c *CacheInvalidator) processCDCEvent(ctx context.Context, event CDCOrganizationEvent) error {
	var tenantID, code string
	
	// æ ¹æ®æ“ä½œç±»å‹è·å–ç§Ÿæˆ·IDå’Œç»„ç»‡ä»£ç 
	switch event.Op {
	case "c", "u": // CREATE, UPDATE
		if event.After == nil {
			return fmt.Errorf("CDC %säº‹ä»¶ç¼ºå°‘afteræ•°æ®", event.Op)
		}
		if event.After.TenantID != nil {
			tenantID = *event.After.TenantID
		}
		if event.After.Code != nil {
			code = *event.After.Code
		}
	case "d": // DELETE
		if event.Before == nil {
			return fmt.Errorf("CDC DELETEäº‹ä»¶ç¼ºå°‘beforeæ•°æ®")
		}
		if event.Before.TenantID != nil {
			tenantID = *event.Before.TenantID
		}
		if event.Before.Code != nil {
			code = *event.Before.Code
		}
	default:
		c.logger.Printf("âš ï¸ æœªçŸ¥çš„CDCæ“ä½œç±»å‹: %s", event.Op)
		return nil
	}

	if tenantID == "" || code == "" {
		c.logger.Printf("âš ï¸ CDCäº‹ä»¶ç¼ºå°‘å¿…è¦ä¿¡æ¯ - ç§Ÿæˆ·ID: %s, ç»„ç»‡ä»£ç : %s", tenantID, code)
		return nil
	}

	c.logger.Printf("ğŸ”„ å¤„ç†CDCäº‹ä»¶ - æ“ä½œ: %s, ç§Ÿæˆ·: %s, ç»„ç»‡: %s", event.Op, tenantID, code)
	
	// å¤±æ•ˆç›¸å…³ç¼“å­˜
	return c.invalidateOrganizationCaches(ctx, tenantID, code)
}

// å¤„ç†Kafkaæ¶ˆæ¯
func (c *CacheInvalidator) processMessage(ctx context.Context, msg *kafka.Message) error {
	topic := *msg.TopicPartition.Topic

	// åªå¤„ç†ç»„ç»‡å•å…ƒCDCäº‹ä»¶
	if topic != "organization_db.public.organization_units" {
		return nil
	}

	c.logger.Printf("ğŸ“¨ æ”¶åˆ°CDCäº‹ä»¶æ¶ˆæ¯ - Topic: %s, Partition: %d, Offset: %d", 
		topic, msg.TopicPartition.Partition, msg.TopicPartition.Offset)

	// è§£æDebeziumæ¶ˆæ¯æ ¼å¼
	var debeziumMsg struct {
		Payload CDCOrganizationEvent `json:"payload"`
	}
	if err := json.Unmarshal(msg.Value, &debeziumMsg); err != nil {
		return fmt.Errorf("ååºåˆ—åŒ–Debeziumæ¶ˆæ¯å¤±è´¥: %w", err)
	}

	return c.processCDCEvent(ctx, debeziumMsg.Payload)
}

// å¼€å§‹æ¶ˆè´¹
func (c *CacheInvalidator) StartConsuming(ctx context.Context) error {
	// è®¢é˜…CDCä¸»é¢˜
	topics := []string{"organization_db.public.organization_units"}
	if err := c.consumer.SubscribeTopics(topics, nil); err != nil {
		return fmt.Errorf("è®¢é˜…Kafkaä¸»é¢˜å¤±è´¥: %w", err)
	}

	c.logger.Printf("ğŸš€ ç¼“å­˜å¤±æ•ˆæœåŠ¡å¼€å§‹è¿è¡Œ...")
	c.logger.Printf("ç›‘å¬ä¸»é¢˜: %v", topics)

	for {
		select {
		case <-ctx.Done():
			c.logger.Println("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢æ¶ˆè´¹...")
			return nil
		default:
			msg, err := c.consumer.ReadMessage(1000)
			if err != nil {
				if err.(kafka.Error).Code() == kafka.ErrTimedOut {
					continue
				}
				c.logger.Printf("æ¶ˆè´¹æ¶ˆæ¯å¤±è´¥: %v", err)
				continue
			}

			if err := c.processMessage(ctx, msg); err != nil {
				c.logger.Printf("å¤„ç†æ¶ˆæ¯å¤±è´¥: %v", err)
			}
		}
	}
}

// å…³é—­èµ„æº
func (c *CacheInvalidator) Close() error {
	var errs []error
	
	if c.consumer != nil {
		if err := c.consumer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("å…³é—­Kafkaæ¶ˆè´¹è€…å¤±è´¥: %w", err))
		}
	}
	
	if c.redisClient != nil {
		if err := c.redisClient.Close(); err != nil {
			errs = append(errs, fmt.Errorf("å…³é—­Redisè¿æ¥å¤±è´¥: %w", err))
		}
	}
	
	if len(errs) > 0 {
		return fmt.Errorf("å…³é—­èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: %v", errs)
	}
	
	return nil
}

func main() {
	logger := log.New(os.Stdout, "[CACHE-INVALIDATOR] ", log.LstdFlags)

	// é…ç½®å‚æ•°
	redisAddr := os.Getenv("REDIS_ADDR")
	if redisAddr == "" {
		redisAddr = "localhost:6379"
	}
	
	redisPassword := os.Getenv("REDIS_PASSWORD")
	
	kafkaBrokers := []string{"localhost:9092"}
	if brokers := os.Getenv("KAFKA_BROKERS"); brokers != "" {
		kafkaBrokers = strings.Split(brokers, ",")
	}
	
	groupID := "cache-invalidator-group"

	// åˆ›å»ºç¼“å­˜å¤±æ•ˆæœåŠ¡
	invalidator, err := NewCacheInvalidator(redisAddr, redisPassword, kafkaBrokers, groupID, logger)
	if err != nil {
		log.Fatalf("åˆ›å»ºç¼“å­˜å¤±æ•ˆæœåŠ¡å¤±è´¥: %v", err)
	}
	defer invalidator.Close()

	// åˆ›å»ºä¸Šä¸‹æ–‡å¤„ç†ä¼˜é›…å…³é—­
	ctx, cancel := context.WithCancel(context.Background())
	
	// ä¼˜é›…å…³é—­
	go func() {
		sigint := make(chan os.Signal, 1)
		signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)
		<-sigint
		
		logger.Println("æ­£åœ¨å…³é—­ç¼“å­˜å¤±æ•ˆæœåŠ¡...")
		cancel()
	}()

	logger.Println("ğŸš€ ç»„ç»‡ç¼“å­˜å¤±æ•ˆæœåŠ¡å¯åŠ¨æˆåŠŸ")
	
	// å¯åŠ¨å¥åº·æ£€æŸ¥æœåŠ¡å™¨
	go startHealthServer(logger)
	
	// å¼€å§‹æ¶ˆè´¹
	if err := invalidator.StartConsuming(ctx); err != nil {
		log.Fatalf("æ¶ˆè´¹å¤±è´¥: %v", err)
	}
	
	logger.Println("ç¼“å­˜å¤±æ•ˆæœåŠ¡å·²å…³é—­")
}

// å¥åº·æ£€æŸ¥æœåŠ¡å™¨
func startHealthServer(logger *log.Logger) {
	mux := http.NewServeMux()
	
	// å¥åº·æ£€æŸ¥ç«¯ç‚¹
	mux.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		response := map[string]interface{}{
			"service": "organization-cache-invalidator",
			"status": "healthy",
			"timestamp": time.Now().Format(time.RFC3339),
			"features": []string{
				"ç²¾ç¡®ç¼“å­˜å¤±æ•ˆ",
				"Redisé›†æˆ", 
				"Kafkaæ¶ˆæ¯æ¶ˆè´¹",
				"CDCäº‹ä»¶å¤„ç†",
			},
		}
		json.NewEncoder(w).Encode(response)
	})
	
	// æŒ‡æ ‡ç«¯ç‚¹
	mux.HandleFunc("/metrics", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/plain")
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("# Cache invalidator metrics\ncache_invalidator_status 1\n"))
	})
	
	server := &http.Server{
		Addr:    ":8082",
		Handler: mux,
	}
	
	logger.Printf("ğŸ” å¥åº·æ£€æŸ¥æœåŠ¡å™¨å¯åŠ¨ - ç«¯å£ 8082")
	if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
		logger.Printf("âŒ å¥åº·æ£€æŸ¥æœåŠ¡å™¨é”™è¯¯: %v", err)
	}
}