package main

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"

	"github.com/gaogu/cube-castle/go-app/internal/neo4j"
	"github.com/google/uuid"
	neo4jdriver "github.com/neo4j/neo4j-go-driver/v5/neo4j"
)

// æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œç›‘æ§
func main() {
	log.Println("âš¡ å¯åŠ¨æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œç›‘æ§...")
	
	// åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
	testEnvironment := setupPerformanceTestEnvironment()
	defer cleanupPerformanceTestEnvironment(testEnvironment)
	
	// æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
	testCases := []struct {
		name     string
		testFunc func(*PerformanceTestEnvironment) error
	}{
		{"åŸºå‡†å»¶è¿Ÿæµ‹è¯•", testBaselineLatency},
		{"ååé‡å‹åŠ›æµ‹è¯•", testThroughputStress},
		{"å¹¶å‘æ€§èƒ½æµ‹è¯•", testConcurrentPerformance},
		{"å†…å­˜å’Œèµ„æºç›‘æ§", testMemoryAndResourceMonitoring},
		{"é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•", testLongTermStability},
		{"æ€§èƒ½å›å½’æµ‹è¯•", testPerformanceRegression},
	}
	
	totalTests := len(testCases)
	passedTests := 0
	
	for _, tc := range testCases {
		log.Printf("ğŸ”„ æ‰§è¡Œæµ‹è¯•: %s", tc.name)
		
		if err := tc.testFunc(testEnvironment); err != nil {
			log.Printf("âŒ æµ‹è¯•å¤±è´¥: %s - %v", tc.name, err)
		} else {
			log.Printf("âœ… æµ‹è¯•é€šè¿‡: %s", tc.name)
			passedTests++
		}
		
		// æµ‹è¯•é—´éš”ï¼Œè®©ç³»ç»Ÿç¨³å®š
		time.Sleep(time.Millisecond * 300)
	}
	
	// è¾“å‡ºæµ‹è¯•ç»“æœ
	log.Printf("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œç›‘æ§å®Œæˆ:")
	log.Printf("   æ€»æµ‹è¯•æ•°: %d", totalTests)
	log.Printf("   é€šè¿‡æµ‹è¯•: %d", passedTests)
	log.Printf("   å¤±è´¥æµ‹è¯•: %d", totalTests-passedTests)
	log.Printf("   æˆåŠŸç‡: %.1f%%", float64(passedTests)/float64(totalTests)*100)
	
	if passedTests == totalTests {
		log.Println("ğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡!")
		log.Println("âœ… ç³»ç»Ÿæ€§èƒ½ç›‘æ§éªŒè¯æˆåŠŸ!")
	} else {
		log.Println("âš ï¸ éƒ¨åˆ†æ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ€§èƒ½ä¼˜åŒ–")
	}
}

// PerformanceTestEnvironment æ€§èƒ½æµ‹è¯•ç¯å¢ƒ
type PerformanceTestEnvironment struct {
	ctx                 context.Context
	highPerformanceManager neo4j.ConnectionManagerInterface
	standardManager     neo4j.ConnectionManagerInterface
	employeeConsumer    *neo4j.EmployeeEventConsumer
	organizationConsumer *neo4j.OrganizationEventConsumer
	
	// æ€§èƒ½ç›‘æ§
	metrics             *PerformanceMetrics
}

// PerformanceMetrics æ€§èƒ½æŒ‡æ ‡
type PerformanceMetrics struct {
	mu                  sync.Mutex
	TotalOperations     int64
	TotalLatency        time.Duration
	MinLatency          time.Duration
	MaxLatency          time.Duration
	OperationsPerSecond float64
	StartTime           time.Time
	LastUpdateTime      time.Time
}

// NewPerformanceMetrics åˆ›å»ºæ€§èƒ½æŒ‡æ ‡ç›‘æ§
func NewPerformanceMetrics() *PerformanceMetrics {
	return &PerformanceMetrics{
		StartTime:      time.Now(),
		LastUpdateTime: time.Now(),
		MinLatency:     time.Hour, // åˆå§‹åŒ–ä¸ºæœ€å¤§å€¼
	}
}

// RecordOperation è®°å½•æ“ä½œæ€§èƒ½
func (pm *PerformanceMetrics) RecordOperation(latency time.Duration) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	
	pm.TotalOperations++
	pm.TotalLatency += latency
	
	if latency < pm.MinLatency {
		pm.MinLatency = latency
	}
	if latency > pm.MaxLatency {
		pm.MaxLatency = latency
	}
	
	// è®¡ç®—æ¯ç§’æ“ä½œæ•°
	elapsed := time.Since(pm.StartTime).Seconds()
	if elapsed > 0 {
		pm.OperationsPerSecond = float64(pm.TotalOperations) / elapsed
	}
	
	pm.LastUpdateTime = time.Now()
}

// GetReport è·å–æ€§èƒ½æŠ¥å‘Š
func (pm *PerformanceMetrics) GetReport() map[string]interface{} {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	
	avgLatency := time.Duration(0)
	if pm.TotalOperations > 0 {
		avgLatency = pm.TotalLatency / time.Duration(pm.TotalOperations)
	}
	
	return map[string]interface{}{
		"total_operations":     pm.TotalOperations,
		"average_latency":      avgLatency.String(),
		"min_latency":          pm.MinLatency.String(),
		"max_latency":          pm.MaxLatency.String(),
		"operations_per_second": pm.OperationsPerSecond,
		"total_time":           time.Since(pm.StartTime).String(),
		"last_update":          pm.LastUpdateTime.Format(time.RFC3339),
	}
}

// setupPerformanceTestEnvironment è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ
func setupPerformanceTestEnvironment() *PerformanceTestEnvironment {
	log.Println("âš¡ è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
	
	ctx := context.Background()
	
	// é«˜æ€§èƒ½é…ç½® - ä½å»¶è¿Ÿé«˜æˆåŠŸç‡
	highPerfConfig := &neo4j.MockConfig{
		SuccessRate:    0.98, // 98%æˆåŠŸç‡
		LatencyMin:     time.Microsecond * 500,
		LatencyMax:     time.Millisecond * 2,
		EnableMetrics:  true,
		ErrorTypes:     []string{"timeout"},
		ErrorRate:      0.02, // 2%é”™è¯¯ç‡
		MaxConnections: 100,
		DatabaseName:   "performance_test_db",
	}
	highPerfManager := neo4j.NewMockConnectionManagerWithConfig(highPerfConfig)
	
	// æ ‡å‡†é…ç½® - åŸºå‡†æ€§èƒ½
	standardConfig := &neo4j.MockConfig{
		SuccessRate:    0.95, // 95%æˆåŠŸç‡
		LatencyMin:     time.Millisecond * 2,
		LatencyMax:     time.Millisecond * 10,
		EnableMetrics:  true,
		ErrorTypes:     []string{"connection_timeout", "transaction_failed"},
		ErrorRate:      0.05, // 5%é”™è¯¯ç‡
		MaxConnections: 50,
		DatabaseName:   "standard_test_db",
	}
	standardManager := neo4j.NewMockConnectionManagerWithConfig(standardConfig)
	
	// åˆ›å»ºäº‹ä»¶æ¶ˆè´¹è€…
	employeeConsumer := neo4j.NewEmployeeEventConsumer(highPerfManager)
	organizationConsumer := neo4j.NewOrganizationEventConsumer(highPerfManager)
	
	// åˆ›å»ºæ€§èƒ½ç›‘æ§
	metrics := NewPerformanceMetrics()
	
	log.Println("âœ… æ€§èƒ½æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
	
	return &PerformanceTestEnvironment{
		ctx:                    ctx,
		highPerformanceManager: highPerfManager,
		standardManager:        standardManager,
		employeeConsumer:       employeeConsumer,
		organizationConsumer:   organizationConsumer,
		metrics:                metrics,
	}
}

// cleanupPerformanceTestEnvironment æ¸…ç†æ€§èƒ½æµ‹è¯•ç¯å¢ƒ
func cleanupPerformanceTestEnvironment(env *PerformanceTestEnvironment) {
	log.Println("ğŸ§¹ æ¸…ç†æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
	
	if env.highPerformanceManager != nil {
		env.highPerformanceManager.Close(env.ctx)
	}
	if env.standardManager != nil {
		env.standardManager.Close(env.ctx)
	}
	
	// è¾“å‡ºæœ€ç»ˆæ€§èƒ½æŠ¥å‘Š
	finalReport := env.metrics.GetReport()
	log.Println("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š:")
	for key, value := range finalReport {
		log.Printf("   %s: %v", key, value)
	}
	
	log.Println("âœ… æ€§èƒ½æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")
}

// testBaselineLatency åŸºå‡†å»¶è¿Ÿæµ‹è¯•
func testBaselineLatency(env *PerformanceTestEnvironment) error {
	log.Println("  â±ï¸ æµ‹è¯•åŸºå‡†å»¶è¿Ÿ...")
	
	// é¢„çƒ­æ“ä½œ
	log.Println("    é¢„çƒ­ç³»ç»Ÿ...")
	for i := 0; i < 10; i++ {
		env.highPerformanceManager.ExecuteRead(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
			return "warmup", nil
		})
	}
	
	// åŸºå‡†å»¶è¿Ÿæµ‹è¯•
	iterations := 100
	var latencies []time.Duration
	
	log.Printf("    æ‰§è¡Œ %d æ¬¡åŸºå‡†æ“ä½œ...", iterations)
	for i := 0; i < iterations; i++ {
		start := time.Now()
		
		_, err := env.highPerformanceManager.ExecuteWrite(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
			return fmt.Sprintf("baseline_%d", i), nil
		})
		
		latency := time.Since(start)
		latencies = append(latencies, latency)
		env.metrics.RecordOperation(latency)
		
		if err != nil && len(latencies) > iterations/2 {
			// å¦‚æœè¶…è¿‡ä¸€åŠçš„æ“ä½œæˆåŠŸï¼Œå…è®¸ä¸€äº›å¤±è´¥
			continue
		}
	}
	
	// è®¡ç®—ç»Ÿè®¡
	if len(latencies) == 0 {
		return fmt.Errorf("æ²¡æœ‰æˆåŠŸçš„æ“ä½œç”¨äºåŸºå‡†æµ‹è¯•")
	}
	
	var totalLatency time.Duration
	minLatency := latencies[0]
	maxLatency := latencies[0]
	
	for _, latency := range latencies {
		totalLatency += latency
		if latency < minLatency {
			minLatency = latency
		}
		if latency > maxLatency {
			maxLatency = latency
		}
	}
	
	avgLatency := totalLatency / time.Duration(len(latencies))
	
	log.Printf("  ğŸ“Š åŸºå‡†å»¶è¿Ÿç»Ÿè®¡:")
	log.Printf("    æˆåŠŸæ“ä½œ: %d/%d", len(latencies), iterations)
	log.Printf("    å¹³å‡å»¶è¿Ÿ: %v", avgLatency)
	log.Printf("    æœ€å°å»¶è¿Ÿ: %v", minLatency)
	log.Printf("    æœ€å¤§å»¶è¿Ÿ: %v", maxLatency)
	
	// æ€§èƒ½é˜ˆå€¼éªŒè¯ï¼ˆåœ¨Mockç¯å¢ƒä¸‹ç›¸å¯¹å®½æ¾ï¼‰
	if avgLatency > time.Millisecond*50 {
		return fmt.Errorf("å¹³å‡å»¶è¿Ÿè¿‡é«˜: %v > 50ms", avgLatency)
	}
	
	log.Println("  âœ… åŸºå‡†å»¶è¿Ÿæµ‹è¯•é€šè¿‡")
	return nil
}

// testThroughputStress ååé‡å‹åŠ›æµ‹è¯•
func testThroughputStress(env *PerformanceTestEnvironment) error {
	log.Println("  ğŸš€ æµ‹è¯•ååé‡å‹åŠ›...")
	
	duration := time.Second * 5 // 5ç§’å‹åŠ›æµ‹è¯•
	concurrency := 10           // 10ä¸ªå¹¶å‘goroutine
	
	log.Printf("    æ‰§è¡Œ %v å‹åŠ›æµ‹è¯•ï¼Œå¹¶å‘åº¦: %d", duration, concurrency)
	
	var wg sync.WaitGroup
	operationCount := int64(0)
	successCount := int64(0)
	var mu sync.Mutex
	
	startTime := time.Now()
	
	// å¯åŠ¨å¹¶å‘å‹åŠ›æµ‹è¯•
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for time.Since(startTime) < duration {
				opStart := time.Now()
				
				_, err := env.highPerformanceManager.ExecuteWrite(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
					return fmt.Sprintf("stress_%d_%d", workerID, time.Now().UnixNano()), nil
				})
				
				opLatency := time.Since(opStart)
				env.metrics.RecordOperation(opLatency)
				
				mu.Lock()
				operationCount++
				if err == nil {
					successCount++
				}
				mu.Unlock()
				
				// çŸ­æš‚ä¼‘æ¯é¿å…è¿‡åº¦å‹åŠ›
				time.Sleep(time.Microsecond * 100)
			}
		}(i)
	}
	
	wg.Wait()
	totalTime := time.Since(startTime)
	
	// è®¡ç®—ååé‡
	throughput := float64(operationCount) / totalTime.Seconds()
	successRate := float64(successCount) / float64(operationCount) * 100
	
	log.Printf("  ğŸ“Š ååé‡å‹åŠ›æµ‹è¯•ç»“æœ:")
	log.Printf("    æ€»æ“ä½œæ•°: %d", operationCount)
	log.Printf("    æˆåŠŸæ“ä½œ: %d", successCount)
	log.Printf("    æˆåŠŸç‡: %.1f%%", successRate)
	log.Printf("    æµ‹è¯•æ—¶é—´: %v", totalTime)
	log.Printf("    ååé‡: %.2f ops/sec", throughput)
	
	// æ€§èƒ½é˜ˆå€¼éªŒè¯
	if throughput < 10.0 { // è‡³å°‘10 ops/sec
		return fmt.Errorf("ååé‡è¿‡ä½: %.2f < 10 ops/sec", throughput)
	}
	
	if successRate < 70.0 { // è‡³å°‘70%æˆåŠŸç‡
		return fmt.Errorf("æˆåŠŸç‡è¿‡ä½: %.1f%% < 70%%", successRate)
	}
	
	log.Println("  âœ… ååé‡å‹åŠ›æµ‹è¯•é€šè¿‡")
	return nil
}

// testConcurrentPerformance å¹¶å‘æ€§èƒ½æµ‹è¯•
func testConcurrentPerformance(env *PerformanceTestEnvironment) error {
	log.Println("  âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
	
	// æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
	concurrencyLevels := []int{1, 5, 10, 20}
	results := make(map[int]PerformanceBenchmark)
	
	for _, concurrency := range concurrencyLevels {
		log.Printf("    æµ‹è¯•å¹¶å‘çº§åˆ«: %d", concurrency)
		
		benchmark := performConcurrentBenchmark(env, concurrency, 100) // æ¯ä¸ªlevel 100ä¸ªæ“ä½œ
		results[concurrency] = benchmark
		
		log.Printf("      å¹³å‡å»¶è¿Ÿ: %v", benchmark.AvgLatency)
		log.Printf("      ååé‡: %.2f ops/sec", benchmark.Throughput)
		log.Printf("      æˆåŠŸç‡: %.1f%%", benchmark.SuccessRate)
		
		time.Sleep(time.Millisecond * 100) // ç¨ä½œä¼‘æ¯
	}
	
	// åˆ†æå¹¶å‘æ€§èƒ½è¶‹åŠ¿
	log.Println("  ğŸ“Š å¹¶å‘æ€§èƒ½åˆ†æ:")
	
	prevThroughput := float64(0)
	for _, concurrency := range concurrencyLevels {
		benchmark := results[concurrency]
		log.Printf("    å¹¶å‘åº¦ %d: ååé‡ %.2f, å»¶è¿Ÿ %v, æˆåŠŸç‡ %.1f%%", 
			concurrency, benchmark.Throughput, benchmark.AvgLatency, benchmark.SuccessRate)
		
		// æ£€æŸ¥å¹¶å‘æ‰©å±•æ€§ï¼ˆå…è®¸ä¸€å®šçš„æ€§èƒ½ä¸‹é™ï¼‰
		if prevThroughput > 0 && benchmark.Throughput < prevThroughput*0.5 {
			log.Printf("    âš ï¸ å¹¶å‘åº¦ %d æ—¶ååé‡æ˜¾è‘—ä¸‹é™", concurrency)
		}
		
		prevThroughput = benchmark.Throughput
	}
	
	log.Println("  âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•é€šè¿‡")
	return nil
}

// PerformanceBenchmark æ€§èƒ½åŸºå‡†ç»“æœ
type PerformanceBenchmark struct {
	Concurrency   int
	TotalOps      int64
	SuccessOps    int64
	TotalTime     time.Duration
	AvgLatency    time.Duration
	Throughput    float64
	SuccessRate   float64
}

// performConcurrentBenchmark æ‰§è¡Œå¹¶å‘åŸºå‡†æµ‹è¯•
func performConcurrentBenchmark(env *PerformanceTestEnvironment, concurrency int, opsPerWorker int) PerformanceBenchmark {
	var wg sync.WaitGroup
	var mu sync.Mutex
	
	totalOps := int64(0)
	successOps := int64(0)
	totalLatency := time.Duration(0)
	
	startTime := time.Now()
	
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for j := 0; j < opsPerWorker; j++ {
				opStart := time.Now()
				
				_, err := env.highPerformanceManager.ExecuteRead(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
					return fmt.Sprintf("concurrent_%d_%d", workerID, j), nil
				})
				
				opLatency := time.Since(opStart)
				
				mu.Lock()
				totalOps++
				totalLatency += opLatency
				if err == nil {
					successOps++
				}
				mu.Unlock()
			}
		}(i)
	}
	
	wg.Wait()
	elapsedTime := time.Since(startTime)
	
	avgLatency := time.Duration(0)
	if totalOps > 0 {
		avgLatency = totalLatency / time.Duration(totalOps)
	}
	
	throughput := float64(totalOps) / elapsedTime.Seconds()
	successRate := float64(successOps) / float64(totalOps) * 100
	
	return PerformanceBenchmark{
		Concurrency: concurrency,
		TotalOps:    totalOps,
		SuccessOps:  successOps,
		TotalTime:   elapsedTime,
		AvgLatency:  avgLatency,
		Throughput:  throughput,
		SuccessRate: successRate,
	}
}

// testMemoryAndResourceMonitoring å†…å­˜å’Œèµ„æºç›‘æ§æµ‹è¯•
func testMemoryAndResourceMonitoring(env *PerformanceTestEnvironment) error {
	log.Println("  ğŸ’¾ æµ‹è¯•å†…å­˜å’Œèµ„æºç›‘æ§...")
	
	// è·å–åˆå§‹ç»Ÿè®¡
	initialStats := env.highPerformanceManager.GetStatistics()
	log.Printf("    åˆå§‹èµ„æºçŠ¶æ€: %+v", initialStats)
	
	// æ‰§è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ
	iterations := 200
	log.Printf("    æ‰§è¡Œ %d æ¬¡å†…å­˜å¯†é›†æ“ä½œ...", iterations)
	
	for i := 0; i < iterations; i++ {
		// åˆ›å»ºå¤§é‡äº‹ä»¶è¿›è¡Œå¤„ç†
		event := &MockDomainEvent{
			EventID:      uuid.New(),
			EventType:    "employee.created",
			AggregateID:  uuid.New(),
			TenantID:     uuid.New(),
			Timestamp:    time.Now(),
			EventVersion: "1.0",
			Payload: map[string]interface{}{
				"employee_number": fmt.Sprintf("MEM%04d", i),
				"first_name":      "å†…å­˜",
				"last_name":       "æµ‹è¯•",
				"email":           fmt.Sprintf("memory%d@test.com", i),
				"data":            generateLargePayload(1024), // 1KB payload
			},
		}
		
		err := env.employeeConsumer.ConsumeEvent(env.ctx, event)
		if err != nil && i > iterations/2 {
			// å…è®¸ä¸€äº›å¤±è´¥ï¼Œåªè¦ä¸æ˜¯å¤§éƒ¨åˆ†éƒ½å¤±è´¥
			continue
		}
		
		// æ¯50æ¬¡æ“ä½œæ£€æŸ¥ä¸€æ¬¡èµ„æºçŠ¶æ€
		if i%50 == 0 {
			stats := env.highPerformanceManager.GetStatistics()
			if totalOps, ok := stats["total_operations"]; ok {
				log.Printf("    æ“ä½œ %d: æ€»æ“ä½œæ•° %v", i, totalOps)
			}
		}
	}
	
	// è·å–æœ€ç»ˆç»Ÿè®¡
	finalStats := env.highPerformanceManager.GetStatistics()
	
	log.Println("  ğŸ“Š èµ„æºç›‘æ§ç»“æœ:")
	log.Printf("    åˆå§‹æ“ä½œæ•°: %v", initialStats["total_operations"])
	log.Printf("    æœ€ç»ˆæ“ä½œæ•°: %v", finalStats["total_operations"])
	log.Printf("    å¹³å‡å»¶è¿Ÿ: %v", finalStats["average_latency"])
	log.Printf("    é”™è¯¯ç‡: %v", finalStats["error_rate"])
	
	log.Println("  âœ… å†…å­˜å’Œèµ„æºç›‘æ§æµ‹è¯•é€šè¿‡")
	return nil
}

// testLongTermStability é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
func testLongTermStability(env *PerformanceTestEnvironment) error {
	log.Println("  â³ æµ‹è¯•é•¿æ—¶é—´ç¨³å®šæ€§...")
	
	duration := time.Second * 10 // 10ç§’ç¨³å®šæ€§æµ‹è¯•
	reportInterval := time.Second * 2 // æ¯2ç§’æŠ¥å‘Šä¸€æ¬¡
	
	log.Printf("    æ‰§è¡Œ %v ç¨³å®šæ€§æµ‹è¯•...", duration)
	
	var wg sync.WaitGroup
	stopChan := make(chan struct{})
	
	// æ€§èƒ½ç›‘æ§goroutine
	wg.Add(1)
	go func() {
		defer wg.Done()
		ticker := time.NewTicker(reportInterval)
		defer ticker.Stop()
		
		for {
			select {
			case <-ticker.C:
				stats := env.highPerformanceManager.GetStatistics()
				log.Printf("    ç¨³å®šæ€§æ£€æŸ¥: æ“ä½œæ•°=%v, å¹³å‡å»¶è¿Ÿ=%v, é”™è¯¯ç‡=%v", 
					stats["total_operations"], stats["average_latency"], stats["error_rate"])
			case <-stopChan:
				return
			}
		}
	}()
	
	// æŒç»­æ“ä½œgoroutine
	wg.Add(1)
	go func() {
		defer wg.Done()
		
		operationCounter := 0
		startTime := time.Now()
		
		for time.Since(startTime) < duration {
			operationCounter++
			
			_, err := env.standardManager.ExecuteWrite(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
				return fmt.Sprintf("stability_%d", operationCounter), nil
			})
			
			if err != nil && operationCounter%10 == 0 {
				log.Printf("    æ“ä½œ %d å¤±è´¥: %v", operationCounter, err)
			}
			
			time.Sleep(time.Millisecond * 50) // ç¨³å®šçš„æ“ä½œé—´éš”
		}
		
		log.Printf("    ç¨³å®šæ€§æµ‹è¯•å®Œæˆï¼Œæ€»æ“ä½œæ•°: %d", operationCounter)
	}()
	
	// ç­‰å¾…æµ‹è¯•å®Œæˆ
	time.Sleep(duration)
	close(stopChan)
	wg.Wait()
	
	log.Println("  âœ… é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•é€šè¿‡")
	return nil
}

// testPerformanceRegression æ€§èƒ½å›å½’æµ‹è¯•
func testPerformanceRegression(env *PerformanceTestEnvironment) error {
	log.Println("  ğŸ“ˆ æµ‹è¯•æ€§èƒ½å›å½’...")
	
	// åŸºå‡†æ€§èƒ½æµ‹è¯•
	log.Println("    æ‰§è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•...")
	baselineBenchmark := performConcurrentBenchmark(env, 5, 50)
	
	// æ¨¡æ‹Ÿä¸€äº›ç³»ç»Ÿå˜åŒ–ï¼ˆå¢åŠ ä¸€äº›è´Ÿè½½ï¼‰
	log.Println("    å¢åŠ ç³»ç»Ÿè´Ÿè½½...")
	
	// åœ¨åå°è¿è¡Œä¸€äº›é¢å¤–æ“ä½œæ¨¡æ‹Ÿè´Ÿè½½
	stopLoadChan := make(chan struct{})
	go func() {
		counter := 0
		for {
			select {
			case <-stopLoadChan:
				return
			default:
				counter++
				env.standardManager.ExecuteRead(env.ctx, func(tx neo4jdriver.ManagedTransaction) (any, error) {
					return fmt.Sprintf("background_load_%d", counter), nil
				})
				time.Sleep(time.Millisecond * 10)
			}
		}
	}()
	
	// è´Ÿè½½ä¸‹çš„æ€§èƒ½æµ‹è¯•
	time.Sleep(time.Millisecond * 500) // è®©è´Ÿè½½è¿è¡Œä¸€æ®µæ—¶é—´
	log.Println("    æ‰§è¡Œè´Ÿè½½ä¸‹æ€§èƒ½æµ‹è¯•...")
	loadedBenchmark := performConcurrentBenchmark(env, 5, 50)
	
	close(stopLoadChan)
	
	// æ€§èƒ½å›å½’åˆ†æ
	log.Println("  ğŸ“Š æ€§èƒ½å›å½’åˆ†æ:")
	log.Printf("    åŸºå‡†ååé‡: %.2f ops/sec", baselineBenchmark.Throughput)
	log.Printf("    è´Ÿè½½ä¸‹ååé‡: %.2f ops/sec", loadedBenchmark.Throughput)
	log.Printf("    åŸºå‡†å»¶è¿Ÿ: %v", baselineBenchmark.AvgLatency)
	log.Printf("    è´Ÿè½½ä¸‹å»¶è¿Ÿ: %v", loadedBenchmark.AvgLatency)
	
	// è®¡ç®—æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”
	throughputDrop := (baselineBenchmark.Throughput - loadedBenchmark.Throughput) / baselineBenchmark.Throughput * 100
	latencyIncrease := float64(loadedBenchmark.AvgLatency - baselineBenchmark.AvgLatency) / float64(baselineBenchmark.AvgLatency) * 100
	
	log.Printf("    ååé‡ä¸‹é™: %.1f%%", throughputDrop)
	log.Printf("    å»¶è¿Ÿå¢åŠ : %.1f%%", latencyIncrease)
	
	// å›å½’æ£€æŸ¥ï¼ˆå…è®¸ä¸€å®šçš„æ€§èƒ½ä¸‹é™ï¼‰
	if throughputDrop > 50.0 {
		log.Printf("    âš ï¸ ååé‡ä¸‹é™è¿‡å¤š: %.1f%%", throughputDrop)
	}
	
	if latencyIncrease > 100.0 {
		log.Printf("    âš ï¸ å»¶è¿Ÿå¢åŠ è¿‡å¤š: %.1f%%", latencyIncrease)
	}
	
	log.Println("  âœ… æ€§èƒ½å›å½’æµ‹è¯•é€šè¿‡")
	return nil
}

// generateLargePayload ç”Ÿæˆå¤§è´Ÿè½½æ•°æ®
func generateLargePayload(sizeKB int) map[string]interface{} {
	payload := make(map[string]interface{})
	
	// ç”ŸæˆæŒ‡å®šå¤§å°çš„æ•°æ®
	dataSize := sizeKB * 1024 / 4 // å¤§çº¦æ¯ä¸ªå­—ç¬¦4å­—èŠ‚
	largeString := make([]byte, dataSize)
	for i := range largeString {
		largeString[i] = byte('A' + (i % 26))
	}
	
	payload["large_data"] = string(largeString)
	payload["metadata"] = map[string]interface{}{
		"size_kb": sizeKB,
		"generated_at": time.Now().Unix(),
	}
	
	return payload
}

// MockDomainEvent æµ‹è¯•ç”¨çš„åŸŸäº‹ä»¶å®ç°ï¼ˆæ€§èƒ½æµ‹è¯•ç‰ˆï¼‰
type MockDomainEvent struct {
	EventID      uuid.UUID
	EventType    string
	AggregateID  uuid.UUID
	TenantID     uuid.UUID
	Timestamp    time.Time
	EventVersion string
	Payload      map[string]interface{}
}

func (e *MockDomainEvent) GetEventID() uuid.UUID     { return e.EventID }
func (e *MockDomainEvent) GetEventType() string      { return e.EventType }
func (e *MockDomainEvent) GetEventVersion() string   { return e.EventVersion }
func (e *MockDomainEvent) GetAggregateID() uuid.UUID { return e.AggregateID }
func (e *MockDomainEvent) GetAggregateType() string  { return "MockAggregate" }
func (e *MockDomainEvent) GetTenantID() uuid.UUID    { return e.TenantID }
func (e *MockDomainEvent) GetTimestamp() time.Time   { return e.Timestamp }
func (e *MockDomainEvent) GetOccurredAt() time.Time  { return e.Timestamp }

func (e *MockDomainEvent) Serialize() ([]byte, error) {
	// ç®€åŒ–çš„åºåˆ—åŒ–ï¼Œä¸“æ³¨äºæ€§èƒ½
	return []byte(fmt.Sprintf("perf_event_%s_%s", e.EventType, e.EventID.String())), nil
}

func (e *MockDomainEvent) GetHeaders() map[string]string {
	return map[string]string{
		"content-type": "application/json",
		"event-type":   e.EventType,
	}
}

func (e *MockDomainEvent) GetMetadata() map[string]interface{} {
	return map[string]interface{}{
		"source":    "performance_test",
		"timestamp": e.Timestamp.Unix(),
	}
}

func (e *MockDomainEvent) GetCorrelationID() string { return "perf-correlation-" + e.EventID.String() }
func (e *MockDomainEvent) GetCausationID() string   { return "perf-causation-" + e.EventID.String() }